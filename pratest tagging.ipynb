{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899c860-673f-4185-a19a-776d03558ddb",
   "metadata": {},
   "source": [
    "# Experiment 13\n",
    "  # Write a NLP Program to demostrate following tasks\n",
    "   # a. Tokenization removal of stop words, punchuation, POS & NER Tags\n",
    "   # b. Bag of Words, TF-IDF Vectorisation & Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c80b25a-ff76-42ad-9316-e0143e7c4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9f8e9f-74e0-4749-a556-559aa2137cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d280f80-83bb-44b5-9dd8-7e1c87e7dd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03383667-9247-42db-971f-fcf0b93f460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Apple is looking at buying U.K. startup for $1 billion. \n",
    "          Artificial Intelligence is the future of technology!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334db66-dd83-4e73-bd53-a356798adf4b",
   "metadata": {},
   "source": [
    "# Part A: Tokenization, Stop Words & Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e08879-9216-4ab7-97e0-f51a07da1233",
   "metadata": {},
   "source": [
    "# Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1082111f-610c-4171-b312-b92dd597ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokens ---\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'Artificial', 'Intelligence', 'is', 'the', 'future', 'of', 'technology', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"--- Tokens ---\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a25146-8420-4b5c-a66d-a086ba37ee57",
   "metadata": {},
   "source": [
    "# Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b7bd5e-4b39-4699-a978-bfa3920dac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_regex = re.compile(r'[\\W_]+')  # matches punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4d58a8-4844-4b1f-ac6c-17df160b1dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokens after Stopword & Punctuation Removal ---\n",
      "['Apple', 'looking', 'buying', 'U.K.', 'startup', '1', 'billion', 'Artificial', 'Intelligence', 'future', 'technology']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and not punctuation_regex.match(word)]\n",
    "print(\"\\n--- Tokens after Stopword & Punctuation Removal ---\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de7549-2be3-4c82-8bc3-f10e0161cf1b",
   "metadata": {},
   "source": [
    "# POS Tagging & NER using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cafc18-0df5-4d89-871a-ddf087913ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # Load small English model\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846fded-79ce-4bd6-af02-9973a9c8910d",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500c8765-8f7d-4959-942a-d55c735d97e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- POS Tagging ---\n",
      "Apple        --> PROPN\n",
      "is           --> AUX\n",
      "looking      --> VERB\n",
      "at           --> ADP\n",
      "buying       --> VERB\n",
      "U.K.         --> PROPN\n",
      "startup      --> VERB\n",
      "for          --> ADP\n",
      "$            --> SYM\n",
      "1            --> NUM\n",
      "billion      --> NUM\n",
      ".            --> PUNCT\n",
      "\n",
      "            --> SPACE\n",
      "Artificial   --> PROPN\n",
      "Intelligence --> PROPN\n",
      "is           --> AUX\n",
      "the          --> DET\n",
      "future       --> NOUN\n",
      "of           --> ADP\n",
      "technology   --> NOUN\n",
      "!            --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- POS Tagging ---\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} --> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb3552-31ce-4962-81e3-4c31db0d8306",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4819d61-8f36-41a7-816a-1c6249940049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Named Entities ---\n",
      "Apple           --> ORG\n",
      "U.K.            --> GPE\n",
      "$1 billion      --> MONEY\n",
      "Artificial Intelligence --> PERSON\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Named Entities ---\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<15} --> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac2753-c294-4ea2-968c-24354db74781",
   "metadata": {},
   "source": [
    "# Part B: Bag of Words & TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de158acc-bbaa-4314-a78a-481eed872951",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d1b9e0-6e05-4887-a14c-cfdf0463fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[1 1 1 1 1 1 1 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([text])\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46622ae-4c48-4629-9543-231979c38f92",
   "metadata": {},
   "source": [
    "#  TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967a70c2-cd04-40d7-bb60-b08fb24a60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563 0.48507125 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text])\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ff98b-b8c2-49bb-908d-69b920ac7413",
   "metadata": {},
   "source": [
    "# N-grams (Bigrams and Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab160a2-bbe2-489f-91fd-c6dd3c37e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- N-grams (Bigrams and Trigrams) ---\n",
      "['apple is' 'apple is looking' 'artificial intelligence'\n",
      " 'artificial intelligence is' 'at buying' 'at buying startup'\n",
      " 'billion artificial' 'billion artificial intelligence' 'buying startup'\n",
      " 'buying startup for' 'for billion' 'for billion artificial' 'future of'\n",
      " 'future of technology' 'intelligence is' 'intelligence is the'\n",
      " 'is looking' 'is looking at' 'is the' 'is the future' 'looking at'\n",
      " 'looking at buying' 'of technology' 'startup for' 'startup for billion'\n",
      " 'the future' 'the future of']\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "ngrams = ngram_vectorizer.fit_transform([text])\n",
    "print(\"\\n--- N-grams (Bigrams and Trigrams) ---\")\n",
    "print(ngram_vectorizer.get_feature_names_out())\n",
    "print(ngrams.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5910-ebf6-4670-8bed-efe53818ede0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
